{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import (RandomForestRegressor, IsolationForest)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import (f_regression, SelectFromModel)\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures, normalize\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(X, X_test):\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    imp.fit(X)\n",
    "    X = imp.transform(X)\n",
    "    X_test = imp.transform(X_test)\n",
    "    return X, X_test\n",
    "#     imputer = KNNImputer(missing_values=np.nan, n_neighbors=100)\n",
    "#     X = imputer.fit_transform(X)\n",
    "#     X_test = imputer.fit_transform(X_test)\n",
    "#     return X, X_test\n",
    "\n",
    "def remove_outliers(X, y):\n",
    "    iforest = IsolationForest(max_samples=100, random_state=1, contamination='auto')\n",
    "    iforest.fit(X)\n",
    "    print(\"Traing data shape before removed: {}\".format(X.shape))\n",
    "    outlier_pred = iforest.predict(X)\n",
    "    X, y = X[(outlier_pred != -1), :], y[(outlier_pred != -1)]\n",
    "    print(\"Traing data shape after removed: {}\".format(X.shape))\n",
    "    return X, y\n",
    "\n",
    "# def select_features(X, y, X_test, feature_num1=150, feature_num2=100):\n",
    "#     scaler = StandardScaler().fit(X, y)\n",
    "#     X = scaler.transform(X)\n",
    "#     X_test = scaler.transform(X_test)\n",
    "    \n",
    "#     features_scores = f_regression(X, y)[0]\n",
    "#     indices_fr = np.asarray(list(features_scores)).argsort()[-feature_num1:][::-1]\n",
    "    \n",
    "#     rf = RandomForestRegressor(n_jobs=-1, n_estimators=50, random_state=1)\n",
    "#     rf.fit(X, y)\n",
    "#     indices_rf = np.asarray(list(rf.feature_importances_)).argsort()[-feature_num2:][::-1]\n",
    "    \n",
    "#     indices = list(np.union1d(indices_rf, indices_fr))\n",
    "#     X = np.take(X, indices, axis = 1)\n",
    "#     X_test = np.take(X_test, indices, axis = 1)\n",
    "    \n",
    "#     return X, X_test\n",
    "\n",
    "def select_features(X, y, X_test, feature_num1=200, feature_num2=100, feature_num3=832):\n",
    "    scaler = StandardScaler().fit(X, y)\n",
    "    X = scaler.transform(X)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    features_scores = f_regression(X, y)[0]\n",
    "    indices_fregression = np.asarray(list(features_scores)).argsort()[-feature_num1:][::-1]\n",
    "    \n",
    "    rf = RandomForestRegressor(n_jobs=-1, n_estimators=50, random_state=1)\n",
    "    rf.fit(X, y)\n",
    "    indices_randomforest = np.asarray(list(rf.feature_importances_)).argsort()[-feature_num2:][::-1]\n",
    "    \n",
    "#     ls = Lasso(alpha=0.0001, normalize=True)\n",
    "#     polynomial_features = PolynomialFeatures(1)\n",
    "#     X = polynomial_features.fit_transform(X)\n",
    "#     ls.fit(X, y)\n",
    "#     indices_lasso = np.asarray(list(np.abs(ls.coef_))).argsort()[-feature_num3:][::-1]\n",
    "\n",
    "#     indices = list(np.intersect1d(indices_lasso, np.union1d(indices_fregression, indices_randomforest))) \n",
    "\n",
    "    indices = list(np.union1d(indices_fregression, indices_randomforest))\n",
    "    X = np.take(X, indices, axis = 1)\n",
    "    X_test = np.take(X_test, indices, axis = 1)\n",
    "    return X, X_test\n",
    "\n",
    "def coeff_determination(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_data = pd.read_csv('X_train.csv')\n",
    "y_train_data = pd.read_csv('y_train.csv')\n",
    "X_test_data = pd.read_csv('X_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imputation of Missing Values\n",
    "* [Reference](https://scikit-learn.org/stable/modules/impute.html)\n",
    "* We use median of column instead of mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_data, X_test_data = fill_missing_values(X_train_data, X_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_test = np.array(X_test_data)[:,0]\n",
    "X_test = np.array(X_test_data)[:,1:]\n",
    "y = np.array(y_train_data)[:,1]\n",
    "X = np.array(X_train_data)[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Outlier Detection\n",
    "* [reference_sklearn](https://scikit-learn.org/stable/modules/outlier_detection.html)\n",
    "* [reference_in_detail](https://practicaldatascience.co.uk/machine-learning/how-to-use-the-isolation-forest-model-for-outlier-detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing data shape before removed: (1212, 832)\n",
      "Traing data shape after removed: (1194, 832)\n"
     ]
    }
   ],
   "source": [
    "X, y = remove_outliers(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zdh/anaconda3/envs/aml_project/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:289: RuntimeWarning: invalid value encountered in true_divide\n",
      "  correlation_coefficient /= X_norms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing data shape after selection: (1194, 213)\n",
      "Testing data shape after selection: (776, 213)\n"
     ]
    }
   ],
   "source": [
    "X, X_test = select_features(X, y, X_test)\n",
    "print(\"Traing data shape after selection: {}\".format(X.shape))\n",
    "print(\"Testing data shape after selection: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extra Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_and_pred(degree, X_train, y_train, X_test):\n",
    "    Y = y_train\n",
    "    dropout = 0.1\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_dim = 213, kernel_regularizer = regularizers.l2(1), kernel_initializer='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), kernel_initializer='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), kernel_initializer='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), kernel_initializer='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), kernel_initializer='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), kernel_initializer='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), kernel_initializer='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "    model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), kernel_initializer='RandomUniform'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(rate = dropout))\n",
    "\n",
    "    model.add(Dense(1, kernel_initializer='RandomUniform'))\n",
    "    # Compile model\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=[coeff_determination])\n",
    "    # Fit the model\n",
    "    print(\"Start fitting ...\")\n",
    "    model.fit(x=X_train, y=Y, epochs=80, verbose=0, validation_split=0.1, shuffle=True, steps_per_epoch=50, initial_epoch=0, validation_steps=5)\n",
    "    # calculate predictions\n",
    "    print(\"calculate predictions\")\n",
    "    predictions = model.predict(X_test)\n",
    "    return predictions\n",
    "    \n",
    "#     model = Lasso(alpha=0.0001, normalize=True)\n",
    "#     polynomial_features = PolynomialFeatures(degree)\n",
    "#     X_train = polynomial_features.fit_transform(X_train)\n",
    "#     X_test = polynomial_features.fit_transform(X_test)\n",
    "#     model.fit(X_train, y_train/100)\n",
    "#     y_pred = model.predict(X_test)\n",
    "\n",
    "#     model = ExtraTreesRegressor(n_jobs=1, max_depth=None, n_estimators=180, random_state=0, min_samples_split=3, max_features=None)\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test) \n",
    "#     return y_pred\n",
    "\n",
    "\n",
    "\n",
    "def train_k_fold(X, y, fold_num=10):\n",
    "    kf = KFold(n_splits=fold_num, random_state=None, shuffle=False)\n",
    "    kf.get_n_splits(X)\n",
    "    train_score = 0.0\n",
    "    test_score = 0.0\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[test_index]\n",
    "        y_train, y_val = y[train_index], y[test_index]\n",
    "\n",
    "        y_pred = fit_model_and_pred(1, X_train, y_train, X_val)\n",
    "        score = r2_score(y_val, y_pred)\n",
    "\n",
    "        print('The obtained r2 score is : ',score)\n",
    "        train_score += score\n",
    "        test_score += score\n",
    "\n",
    "    print(\"Training score: %f\"%(train_score/fold_num))\n",
    "    print(\"Validation score: %f\"%(test_score/fold_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fitting ...\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4000 batches). You may need to use the repeat() function when building your dataset.\n",
      "calculate predictions\n",
      "The obtained r2 score is :  0.50176112381747\n",
      "Start fitting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zdh/anaconda3/envs/aml_project/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4000 batches). You may need to use the repeat() function when building your dataset.\n",
      "calculate predictions\n",
      "The obtained r2 score is :  0.48153841384958074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zdh/anaconda3/envs/aml_project/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fitting ...\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4000 batches). You may need to use the repeat() function when building your dataset.\n",
      "calculate predictions\n",
      "The obtained r2 score is :  0.3661374284021933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zdh/anaconda3/envs/aml_project/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fitting ...\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4000 batches). You may need to use the repeat() function when building your dataset.\n",
      "calculate predictions\n",
      "The obtained r2 score is :  0.4813365379073593\n",
      "Start fitting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zdh/anaconda3/envs/aml_project/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4000 batches). You may need to use the repeat() function when building your dataset.\n",
      "calculate predictions\n",
      "The obtained r2 score is :  0.4242904700087017\n",
      "Start fitting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zdh/anaconda3/envs/aml_project/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4000 batches). You may need to use the repeat() function when building your dataset.\n",
      "calculate predictions\n",
      "The obtained r2 score is :  0.4403258318997276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zdh/anaconda3/envs/aml_project/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fitting ...\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4000 batches). You may need to use the repeat() function when building your dataset.\n",
      "calculate predictions\n",
      "The obtained r2 score is :  0.387380275548058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zdh/anaconda3/envs/aml_project/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fitting ...\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4000 batches). You may need to use the repeat() function when building your dataset.\n",
      "calculate predictions\n",
      "The obtained r2 score is :  0.37012384117562225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zdh/anaconda3/envs/aml_project/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fitting ...\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4000 batches). You may need to use the repeat() function when building your dataset.\n",
      "calculate predictions\n",
      "The obtained r2 score is :  0.5423794516542533\n",
      "Start fitting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zdh/anaconda3/envs/aml_project/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4000 batches). You may need to use the repeat() function when building your dataset.\n",
      "calculate predictions\n",
      "The obtained r2 score is :  0.3429586251571065\n",
      "Training score: 0.433823\n",
      "Validation score: 0.433823\n"
     ]
    }
   ],
   "source": [
    "train_k_fold(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fitting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zdh/anaconda3/envs/aml_project/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4000 batches). You may need to use the repeat() function when building your dataset.\n",
      "calculate predictions\n"
     ]
    }
   ],
   "source": [
    "Y_test_pred = fit_model_and_pred(1, X, y, X_test)\n",
    "\n",
    "final_res = np.vstack((indices_test, Y_test_pred[:,0])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame(final_res)\n",
    "df_res.to_csv(\"our_result.csv\", header = [\"id\", \"y\"], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
